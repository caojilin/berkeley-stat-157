{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 5 - Berkeley STAT 157\n",
    "\n",
    "**Your name: XX, SID YY** (Please add your name, and SID to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/) instead of Github, so you will get the score distribution for each question. Please enroll in the [class](https://www.gradescope.com/courses/42432) by the Entry code: MXG5G5** \n",
    "\n",
    "Handout 2/19/2019, due 2/26/2019 by 4pm in Git by committing to your repository.\n",
    "\n",
    "In this homework, we will model covariate shift and attempt to fix it using logistic regression. This is a fairly realistic scenario for data scientists. To keep things well under control and understandable we will use [Fashion-MNIST](http://d2l.ai/chapter_linear-networks/fashion-mnist.html) as the data to experiment on. \n",
    "\n",
    "Follow the instructions from the Fashion MNIST notebook to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:29:05.201869Z",
     "start_time": "2019-02-26T21:29:04.691027Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import mxnet as mx\n",
    "\n",
    "mnist_train = gdata.vision.FashionMNIST(train=True)\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon.\n",
    "2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon.\n",
    "3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `shirt` and `sweater` and class $-1$ being the combination of `sandal` and `sneaker` categories. \n",
    "4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. \n",
    "\n",
    "Hint - you should encapsulate the training and reporting code in a callable function since you'll need it quite a bit in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:29:06.929071Z",
     "start_time": "2019-02-26T21:29:06.701422Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logisticLoss(f, y):\n",
    "    return -nd.log(1 + nd.exp(-y * f))\n",
    "\n",
    "y = nd.ones(1)\n",
    "f = nd.arange(-5, 5, 0.1)\n",
    "f.attach_grad()\n",
    "with autograd.record():\n",
    "    l = logisticLoss(f, y)\n",
    "l.backward()\n",
    "\n",
    "plt.plot(f.asnumpy(), f.grad.asnumpy(), label='derivative')\n",
    "plt.plot(f.asnumpy(), l.asnumpy(), label = 'loss value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:06:53.432549Z",
     "start_time": "2019-02-26T22:06:53.424566Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = mnist_train[:][0] \n",
    "train_labels = mnist_train[:][1]\n",
    "test_features = mnist_test[:][0] \n",
    "test_labels = mnist_test[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:29:11.636660Z",
     "start_time": "2019-02-26T21:29:07.951717Z"
    }
   },
   "outputs": [],
   "source": [
    "train_shoes = [nd.array(x, dtype='float32') for x, y in zip(train_features, train_labels) if y == 5 or y == 7]\n",
    "train_cloths = [nd.array(x, dtype='float32') for x, y in zip(train_features, train_labels) if y == 2 or y == 6]\n",
    "train = train_shoes + train_cloths\n",
    "\n",
    "a = nd.repeat(nd.array([-1], dtype='float32'), repeats = 12000)\n",
    "b = nd.repeat(nd.array([1], dtype='float32'), repeats = 12000)\n",
    "train_label = nd.concat(a, b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:29:12.345528Z",
     "start_time": "2019-02-26T21:29:11.640336Z"
    }
   },
   "outputs": [],
   "source": [
    "test_shoes = [nd.array(x, dtype='float32') for x, y in zip(test_features, test_labels) if y == 5 or y == 7]\n",
    "test_cloths = [nd.array(x, dtype='float32') for x, y in zip(test_features, test_labels) if y == 2 or y == 6]\n",
    "test = test_shoes + test_cloths\n",
    "\n",
    "a = nd.repeat(nd.array([-1], dtype='float32'), repeats = 2000)\n",
    "b = nd.repeat(nd.array([1], dtype='float32'), repeats = 2000)\n",
    "test_label = nd.concat(a, b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:29:12.983732Z",
     "start_time": "2019-02-26T21:29:12.977821Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:13:13.085985Z",
     "start_time": "2019-02-26T22:13:13.078305Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_iter(train, train_label, test, test_label ):   \n",
    "    train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        train, train_label), batch_size, shuffle=True)\n",
    "\n",
    "    test_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        test, test_label), batch_size, shuffle=False)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:35:11.424823Z",
     "start_time": "2019-02-26T21:35:11.416795Z"
    }
   },
   "outputs": [],
   "source": [
    "def getnet(): \n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:45:31.489128Z",
     "start_time": "2019-02-26T21:45:31.479746Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "# los = logisticLoss\n",
    "los = gloss.SigmoidBinaryCrossEntropyLoss()\n",
    "net = getnet()\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:45:31.925134Z",
     "start_time": "2019-02-26T21:45:31.916016Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()\n",
    "\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum += (net(X) > 0).sum().asscalar()\n",
    "        n += y.size\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:47:09.174735Z",
     "start_time": "2019-02-26T22:47:06.694974Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(train_iter , test_iter):\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                yhat = net(X)\n",
    "                l = los(yhat, y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "#         print(test_acc)\n",
    "train(train_iter , test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:45:47.629606Z",
     "start_time": "2019-02-26T21:45:47.623820Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# net[0].weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariate Shift\n",
    "\n",
    "Your goal is to introduce covariate shit in the data and observe the accuracy. For this, compose a dataset of $12,000$ observations, given by a mixture of `shirt` and `sweater` and of `sandal` and `sneaker` respectively, where you use a fraction $\\lambda \\in \\{0.05, 0.1, 0.2, \\ldots 0.8, 0.9, 0.95\\}$ of one and a fraction of $1-\\lambda$ of  the other datasets respectively. For instance, you might pick for $\\lambda = 0.1$ a total of $600$ `shirt` and $5,400$ `sweater` images and likewise $600$ `sandal` and $5,400$ `sneaker` photos, yielding a total of $12,000$ images for training. Note that the test set remains unbiased, composed of $2,000$ photos for the `shirt` + `sweater` category and of the `sandal` + `sneaker` category each.\n",
    "\n",
    "1. Generate training sets that are appropriately biased. You should have 11 datasets.\n",
    "2. Train a binary classifier using this and report the test set accuracy on the unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:45:50.634434Z",
     "start_time": "2019-02-26T21:45:50.616375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.05       0.1        0.2        0.3        0.4        0.5\n",
       " 0.6        0.70000005 0.8        0.90000004 0.95      ]\n",
       "<NDArray 11 @cpu(0)>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambd = nd.concat(nd.array([0.05]) , nd.arange(0.1, 1, 0.1), nd.array([0.95]), dim=0)\n",
    "lambd\n",
    "one_minus_lambd = 1 - lambd\n",
    "one_minus_lambd\n",
    "lambd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T21:48:38.609343Z",
     "start_time": "2019-02-26T21:48:38.598149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[5700 5400 4800 4200 3600 3000 2400 1800 1200  600  300]\n",
       "<NDArray 11 @cpu(0)>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fraction = nd.array(6000 * lambd, dtype='int')\n",
    "fraction\n",
    "the_rest = 6000 - fraction\n",
    "the_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:19:55.335896Z",
     "start_time": "2019-02-26T22:18:56.846449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n",
      "6000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "train_label = []\n",
    "\n",
    "for k in range(len(lambd)):\n",
    "    frac = fraction[k].asscalar()\n",
    "    rest =  the_rest[k].asscalar()\n",
    "    \n",
    "    sandal = [nd.array(x, dtype='float32') for x, y in zip(train_features, train_labels) if y == 5]\n",
    "    sandal = sandal[:frac]\n",
    "    \n",
    "    sneaker  = [nd.array(x, dtype='float32') for x, y in zip(train_features, train_labels) if y == 7]\n",
    "    sneaker = sneaker[:rest]\n",
    "    train_shoes = sandal + sneaker\n",
    "    label_shoes = nd.repeat(nd.array([-1], dtype='float32'), repeats = 6000)\n",
    "\n",
    "    \n",
    "    shirt = [nd.array(x, dtype='float32') for x, y in zip(train_features, train_labels) if y == 6]\n",
    "    shirt = shirt[:frac]\n",
    "    \n",
    "    sweater  = [nd.array(x, dtype='float32') for x, y in zip(train_features, train_labels) if y == 2]\n",
    "    sweater = sweater[:rest]\n",
    "    train_cloths = shirt + sweater\n",
    "    label_cloths = nd.repeat(nd.array([1], dtype='float32'), repeats = 6000)\n",
    "    \n",
    "#     print(len(train_shoes))\n",
    "    train_dataset.append(train_shoes + train_cloths)\n",
    "    train_label.append(nd.concat(label_shoes, label_cloths, dim=0))\n",
    "#     print(len(nd.concat(label_shoes, label_cloths, dim=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:15:16.346624Z",
     "start_time": "2019-02-26T22:15:15.699439Z"
    }
   },
   "outputs": [],
   "source": [
    "test_shoes = [nd.array(x, dtype='float32') for x, y in zip(test_features, test_labels) if y == 5 or y == 7]\n",
    "test_cloths = [nd.array(x, dtype='float32') for x, y in zip(test_features, test_labels) if y == 2 or y == 6]\n",
    "test_dataset = test_shoes + test_cloths\n",
    "\n",
    "a = nd.repeat(nd.array([-1], dtype='float32'), repeats = 2000)\n",
    "b = nd.repeat(nd.array([1], dtype='float32'), repeats = 2000)\n",
    "test_label = nd.concat(a, b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:41:31.261633Z",
     "start_time": "2019-02-26T22:41:31.230468Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in range(11):\n",
    "    train_iter, test_iter = get_iter(train_dataset[k], train_label[k], test_dataset, test_label)\n",
    "    los = gloss.SigmoidBinaryCrossEntropyLoss()\n",
    "    net = getnet()\n",
    "    num_epochs = 5\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "    train(train_iter , test_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Shift Correction\n",
    "\n",
    "Having observed that covariate shift can be harmful, let's try fixing it. For this we first need to compute the appropriate propensity scores $\\frac{dp(x)}{dq(x)}$. For this purpose pick a biased dataset, let's say with $\\lambda = 0.1$ and try to fix the covariate shift.\n",
    "\n",
    "1. When training a logistic regression binary classifier to fix covariate shift, we assumed so far that both sets are of equal size. Show that re-weighting data in training and test set appropriately can help address the issue when both datasets have different size. What is the weighting?\n",
    "2. Train a binary classifier (using logistic regression) distinguishing between the biased training set and the unbiased test set. Note - you need to weigh the data. \n",
    "3. Use the scores to compute weights on the training set. Do they match the weight arising from the biasing distribution $\\lambda$? \n",
    "4. Train a binary classifier of the covariate shifted problem using the weights obtained previously and report the accuracy. Note - you will need to modify the training loop slightly such that you can compute the gradient of a weighted sum of losses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $$\\int d x q ( x ) f ( x ) = \\int d x p ( x ) \\underbrace { \\frac { q ( x ) } { p ( x ) } } _ { \\alpha ( x ) } f ( x ) = \\int d x p ( x ) \\alpha ( x ) f ( x ) \\\\\n",
    "r ( x , y ) = \\frac { 1 } { 2 } [ p ( x ) \\delta ( y , 1 ) + q ( x ) \\delta ( y , - 1 ) ]  \\\\\n",
    "r ( y = 1 | x ) = \\frac { p ( x ) } { p ( x ) + q ( x ) } \\text{ and hence } \\alpha = \\frac { q ( x ) } { p ( x ) } = \\frac { r ( y = - 1 | x ) } { r ( y = 1 | x ) } \\\\\n",
    "r ( y = 1 | x ) = \\frac { 1 } { 1 + \\exp ( - f ( x ) ) } \\\\  \n",
    "\\alpha ( x ) = \\frac { r ( y = - 1 | x ) } { r ( y = 1 | x ) } = \\exp ( f ( x ) )$$\n",
    "the weighting is $$exp(f(x_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:47:36.261586Z",
     "start_time": "2019-02-26T22:47:11.107195Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "for k in range(11):\n",
    "    train_iter, test_iter = get_iter(train_dataset[k], train_label[k], test_dataset, test_label)\n",
    "    los = gloss.SigmoidBinaryCrossEntropyLoss()\n",
    "    net = getnet()\n",
    "    num_epochs = 5\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "    train(train_iter , test_iter)\n",
    "    weight = []\n",
    "    for X, y in train_iter:\n",
    "        weight.append(nd.exp(net(X)))\n",
    "    weights.append(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:47:38.462118Z",
     "start_time": "2019-02-26T22:47:38.455481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T22:48:12.845435Z",
     "start_time": "2019-02-26T22:48:12.839572Z"
    }
   },
   "outputs": [],
   "source": [
    "def train2(train_iter , test_iter, weight):\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                yhat = net(X)\n",
    "                l = weight * los(yhat, y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print(test_acc)\n",
    "        \n",
    "for k in range(11):\n",
    "    net = getnet()\n",
    "    train_iter, test_iter = get_iter(train_dataset[k], train_label[k], test_dataset, test_label)\n",
    "    los = gloss.SigmoidBinaryCrossEntropyLoss()\n",
    "    net = getnet()\n",
    "    num_epochs = 5\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "    train2(train_iter, test_iter, weights[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
