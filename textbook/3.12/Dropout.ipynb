{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:12:41.925129Z",
     "start_time": "2019-02-13T05:12:40.744569Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "def dropout(X, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    # In this case, all elements are dropped out\n",
    "    if drop_prob == 1:\n",
    "        return X.zeros_like()\n",
    "    mask = nd.random.uniform(0, 1, X.shape) > drop_prob\n",
    "    return mask * X / (1.0-drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:15:23.537306Z",
     "start_time": "2019-02-13T05:15:23.516651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
      "<NDArray 2x8 @cpu(0)>\n",
      "\n",
      "[[ 0.  0.  0.  0.  8. 10. 12.  0.]\n",
      " [16.  0. 20. 22.  0.  0.  0. 30.]]\n",
      "<NDArray 2x8 @cpu(0)>\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "<NDArray 2x8 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "X = nd.arange(16).reshape((2, 8))\n",
    "print(dropout(X, 0))\n",
    "print(dropout(X, 0.5))\n",
    "print(dropout(X, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:17:12.765633Z",
     "start_time": "2019-02-13T05:17:12.751220Z"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:17:39.218831Z",
     "start_time": "2019-02-13T05:17:39.207475Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X, W1) + b1).relu()\n",
    "    # Use dropout only when training the model\n",
    "    if autograd.is_training():\n",
    "        # Add a dropout layer after the first fully connected layer\n",
    "        H1 = dropout(H1, drop_prob1)\n",
    "    H2 = (nd.dot(H1, W2) + b2).relu()\n",
    "    if autograd.is_training():\n",
    "        # Add a dropout layer after the second fully connected layer\n",
    "        H2 = dropout(H2, drop_prob2)\n",
    "    return nd.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:44:44.972235Z",
     "start_time": "2019-02-13T05:43:42.828732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.3358, train acc 0.876, test acc 0.874\n",
      "epoch 2, loss 0.3298, train acc 0.880, test acc 0.885\n",
      "epoch 3, loss 0.3245, train acc 0.882, test acc 0.879\n",
      "epoch 4, loss 0.3143, train acc 0.884, test acc 0.888\n",
      "epoch 5, loss 0.3093, train acc 0.886, test acc 0.883\n",
      "epoch 6, loss 0.3067, train acc 0.887, test acc 0.886\n",
      "epoch 7, loss 0.2978, train acc 0.890, test acc 0.886\n",
      "epoch 8, loss 0.2916, train acc 0.891, test acc 0.891\n",
      "epoch 9, loss 0.2894, train acc 0.892, test acc 0.887\n",
      "epoch 10, loss 0.2852, train acc 0.894, test acc 0.891\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 10, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:48:49.110432Z",
     "start_time": "2019-02-13T05:47:21.950380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1262, train acc 0.561, test acc 0.746\n",
      "epoch 2, loss 0.5768, train acc 0.787, test acc 0.833\n",
      "epoch 3, loss 0.4928, train acc 0.821, test acc 0.851\n",
      "epoch 4, loss 0.4455, train acc 0.837, test acc 0.860\n",
      "epoch 5, loss 0.4155, train acc 0.849, test acc 0.859\n",
      "epoch 6, loss 0.3943, train acc 0.857, test acc 0.868\n",
      "epoch 7, loss 0.3814, train acc 0.862, test acc 0.872\n",
      "epoch 8, loss 0.3649, train acc 0.866, test acc 0.875\n",
      "epoch 9, loss 0.3560, train acc 0.869, test acc 0.877\n",
      "epoch 10, loss 0.3417, train acc 0.874, test acc 0.869\n",
      "epoch 11, loss 0.3345, train acc 0.877, test acc 0.877\n",
      "epoch 12, loss 0.3267, train acc 0.880, test acc 0.877\n",
      "epoch 13, loss 0.3209, train acc 0.883, test acc 0.888\n",
      "epoch 14, loss 0.3124, train acc 0.885, test acc 0.884\n",
      "epoch 15, loss 0.3078, train acc 0.887, test acc 0.885\n",
      "epoch 16, loss 0.3055, train acc 0.888, test acc 0.889\n",
      "epoch 17, loss 0.2952, train acc 0.891, test acc 0.887\n",
      "epoch 18, loss 0.2925, train acc 0.893, test acc 0.884\n",
      "epoch 19, loss 0.2850, train acc 0.894, test acc 0.878\n",
      "epoch 20, loss 0.2850, train acc 0.894, test acc 0.886\n"
     ]
    }
   ],
   "source": [
    "#Concise Implementation\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "        # Add a dropout layer after the first fully connected layer\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        # Add a dropout layer after the second fully connected layer\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, 20, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
