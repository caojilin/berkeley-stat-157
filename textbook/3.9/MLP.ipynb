{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "%matplotlib inline\n",
    "import d2l\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))\n",
    "b1 = nd.zeros(num_hiddens)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs))\n",
    "b2 = nd.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(nd.dot(X, W1) + b1)\n",
    "    return nd.dot(H, W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.8428, train acc 0.688, test acc 0.815\n",
      "epoch 2, loss 0.4911, train acc 0.819, test acc 0.835\n",
      "epoch 3, loss 0.4338, train acc 0.839, test acc 0.845\n",
      "epoch 4, loss 0.4001, train acc 0.852, test acc 0.861\n",
      "epoch 5, loss 0.3744, train acc 0.862, test acc 0.867\n",
      "epoch 6, loss 0.3537, train acc 0.871, test acc 0.877\n",
      "epoch 7, loss 0.3423, train acc 0.873, test acc 0.878\n",
      "epoch 8, loss 0.3284, train acc 0.879, test acc 0.874\n",
      "epoch 9, loss 0.3222, train acc 0.881, test acc 0.883\n",
      "epoch 10, loss 0.3085, train acc 0.886, test acc 0.884\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 10, 0.5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.8036, train acc 0.702, test acc 0.817\n",
      "epoch 2, loss 0.4907, train acc 0.818, test acc 0.848\n",
      "epoch 3, loss 0.4273, train acc 0.843, test acc 0.858\n",
      "epoch 4, loss 0.3966, train acc 0.853, test acc 0.851\n",
      "epoch 5, loss 0.3725, train acc 0.862, test acc 0.869\n",
      "epoch 6, loss 0.3553, train acc 0.870, test acc 0.863\n",
      "epoch 7, loss 0.3426, train acc 0.874, test acc 0.878\n",
      "epoch 8, loss 0.3269, train acc 0.880, test acc 0.880\n",
      "epoch 9, loss 0.3151, train acc 0.884, test acc 0.884\n",
      "epoch 10, loss 0.3083, train acc 0.887, test acc 0.885\n"
     ]
    }
   ],
   "source": [
    "# concise\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import d2l\n",
    "from mxnet import gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "num_epochs = 10\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
