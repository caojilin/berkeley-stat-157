{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 5 - Berkeley STAT 157\n",
    "\n",
    "**Your name: XX, SID YY** (Please add your name, and SID to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/) instead of Github, so you will get the score distribution for each question. Please enroll in the [class](https://www.gradescope.com/courses/42432) by the Entry code: MXG5G5** \n",
    "\n",
    "Handout 2/19/2019, due 2/26/2019 by 4pm in Git by committing to your repository.\n",
    "\n",
    "In this homework, we will model covariate shift and attempt to fix it using logistic regression. This is a fairly realistic scenario for data scientists. To keep things well under control and understandable we will use [Fashion-MNIST](http://d2l.ai/chapter_linear-networks/fashion-mnist.html) as the data to experiment on. \n",
    "\n",
    "Follow the instructions from the Fashion MNIST notebook to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T23:40:37.652396Z",
     "start_time": "2019-03-01T23:40:33.912263Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import d2l\n",
    "\n",
    "mnist_train = gdata.vision.FashionMNIST(train=True)\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon.\n",
    "2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon.\n",
    "3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `sneaker` and `pullover` and class $-1$ being the combination of `sandal` and `shirt` categories. \n",
    "4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. \n",
    "\n",
    "Hint - you should encapsulate the training and reporting code in a callable function since you'll need it quite a bit in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T23:40:38.100984Z",
     "start_time": "2019-03-01T23:40:37.657656Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1\n",
    "def logisticLoss(f, y):\n",
    "    return -nd.log(1 + nd.exp(-y * f))\n",
    "#2\n",
    "y = nd.ones(1)\n",
    "f = nd.arange(-5, 5, 0.1)\n",
    "f.attach_grad()\n",
    "with autograd.record():\n",
    "    l = logisticLoss(f, y)\n",
    "l.backward()\n",
    "\n",
    "plt.plot(f.asnumpy(), f.grad.asnumpy())\n",
    "plt.plot(f.asnumpy(), l.asnumpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Label\tDescription\n",
    "0\tT-shirt/top\n",
    "1\tTrouser\n",
    "2\tPullover\n",
    "3\tDress\n",
    "4\tCoat\n",
    "5\tSandal\n",
    "6\tShirt\n",
    "7\tSneaker\n",
    "8\tBag\n",
    "9\tAnkle boot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T23:40:46.062994Z",
     "start_time": "2019-03-01T23:40:38.105193Z"
    }
   },
   "outputs": [],
   "source": [
    "#3 \n",
    "def extract(data):\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        feature, label = data[i]\n",
    "        label = d2l.get_fashion_mnist_labels([label])[0]\n",
    "        if label == \"sneaker\" or label == \"pullover\":\n",
    "            dataset.append((data[i][0].astype('float32').reshape(1, 784), nd.array([1])))\n",
    "        elif label == \"sandal\" or label == \"shirt\":\n",
    "            dataset.append((data[i][0].astype('float32').reshape(1, 784), nd.array([-1])))\n",
    "    return dataset\n",
    "\n",
    "new_train = extract(mnist_train)\n",
    "new_test = extract(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T23:40:46.074106Z",
     "start_time": "2019-03-01T23:40:46.067352Z"
    }
   },
   "outputs": [],
   "source": [
    "def getnet():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.945Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(y):\n",
    "    return 1 if y[0].asscalar() > 0 else -1 \n",
    "\n",
    "def get_accuracy(data_iter, net):\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum += (predict(net(X)) == y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar() / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.948Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, loss, num_epochs, batch_size, lr=None):\n",
    "    net = getnet()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "#         print(len(train_iter))\n",
    "        for X, y in train_iter:\n",
    "#             print(X.shape)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (predict(y_hat) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = get_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 6832.9489, train acc 0.795, test acc 0.830\n",
      "epoch 2, loss 6994.4781, train acc 0.822, test acc 0.616\n",
      "epoch 3, loss 7087.2501, train acc 0.832, test acc 0.646\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "batch_size = 256\n",
    "loss = gloss.LogisticLoss()\n",
    "num_epochs, lr = 3, 0.5\n",
    "halved = new_train[:12000]\n",
    "trained_half = train(halved, new_test, loss, num_epochs, batch_size, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 6985.5531, train acc 0.811, test acc 0.873\n",
      "epoch 2, loss 7380.3410, train acc 0.832, test acc 0.873\n",
      "epoch 3, loss 7317.9732, train acc 0.836, test acc 0.880\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 3, 0.5\n",
    "# training takes too long, so one epoch is enough.\n",
    "trained_full = train(new_train, new_test, loss, num_epochs, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic Regression model can perfectly separate two categories. We got same accurary on train and test because two dataset come from same distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariate Shift\n",
    "\n",
    "Your goal is to introduce covariate shit in the data and observe the accuracy. For this, compose a dataset of $12,000$ observations, given by a mixture of `shirt` and `sweater` and of `sandal` and `sneaker` respectively, where you use a fraction $\\lambda \\in \\{0.05, 0.1, 0.2, \\ldots 0.8, 0.9, 0.95\\}$ of one and a fraction of $1-\\lambda$ of  the other datasets respectively. For instance, you might pick for $\\lambda = 0.1$ a total of $600$ `shirt` and $5,400$ `sweater` images and likewise $600$ `sandal` and $5,400$ `sneaker` photos, yielding a total of $12,000$ images for training. Note that the test set remains unbiased, composed of $2,000$ photos for the `shirt` + `sweater` category and of the `sandal` + `sneaker` category each.\n",
    "\n",
    "1. Generate training sets that are appropriately biased. You should have 11 datasets.\n",
    "2. Train a binary classifier using this and report the test set accuracy on the unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.961Z"
    }
   },
   "outputs": [],
   "source": [
    "#1\n",
    "def get_data(frac, indices_map):\n",
    "    num_per_class = len(indices_map[2])\n",
    "    size_class_one = int(frac * num_per_class)\n",
    "    size_class_two = int((1 - frac) * num_per_class)\n",
    "    pullover_indices = np.random.choice(indices_map[2], size=size_class_one, replace = False)\n",
    "    sneaker_indices = np.random.choice(indices_map[7], size=size_class_two, replace = False)\n",
    "    shirts_indices = np.concatenate((pullover_indices, sneaker_indices), axis=0)\n",
    "\n",
    "    sandal_indices = np.random.choice(indices_map[5], size=size_class_two, replace = False)\n",
    "    shirt_indices = np.random.choice(indices_map[6], size=size_class_one, replace = False)\n",
    "    shoes_indices = np.concatenate((sandal_indices, shirt_indices), axis=0)\n",
    "    processed = []\n",
    "    for i in np.concatenate((shirts_indices, shoes_indices), axis=0):\n",
    "        feature, label = mnist_train[i]\n",
    "        if label == 2 or label == 7:\n",
    "            processed.append((mnist_train[i][0].astype('float32').reshape(1, 784), nd.array([1])))\n",
    "        elif label == 5 or label == 6:\n",
    "            processed.append((mnist_train[i][0].astype('float32').reshape(1, 784), nd.array([-1])))\n",
    "    return processed\n",
    "\n",
    "def indices(labels):\n",
    "    indices_map = {}\n",
    "    for i in labels:\n",
    "        indices_map[i] = list()\n",
    "    for i in range(len(mnist_train)):\n",
    "        _, label = mnist_train[i]\n",
    "        if label in indices_map:\n",
    "            indices_map[label].append(i)\n",
    "    return indices_map \n",
    "\n",
    "# 2 = pullover, 5 = sandal, 6 = shirt, 7 = sneaker.\n",
    "labels = [2, 5, 7, 6]\n",
    "indices_map = indices(labels) # indices map stores indices of each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.964Z"
    }
   },
   "outputs": [],
   "source": [
    "splitted_data = []\n",
    "frac = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "for f in frac:\n",
    "    splitted_data.append(get_data(f, indices_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction =  0.05\n",
      "epoch 1, loss 9.2170, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 275.5560, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 558.4436, train acc 0.999, test acc 0.500\n",
      "\n",
      "fraction =  0.1\n",
      "epoch 1, loss 8.3322, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 295.8321, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 628.8992, train acc 0.998, test acc 0.500\n",
      "\n",
      "fraction =  0.2\n",
      "epoch 1, loss 5.8576, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 256.1671, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 521.1045, train acc 0.998, test acc 0.500\n",
      "\n",
      "fraction =  0.3\n",
      "epoch 1, loss 2.4011, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 400.2403, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 614.5046, train acc 0.998, test acc 0.500\n",
      "\n",
      "fraction =  0.4\n",
      "epoch 1, loss 6.6500, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 360.8222, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 578.8242, train acc 0.998, test acc 0.500\n",
      "\n",
      "fraction =  0.5\n",
      "epoch 1, loss 1.9005, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 210.3912, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 358.5376, train acc 0.998, test acc 0.500\n",
      "\n",
      "fraction =  0.6\n",
      "epoch 1, loss 7.6018, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 273.3808, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 483.0471, train acc 0.999, test acc 0.500\n",
      "\n",
      "fraction =  0.7\n",
      "epoch 1, loss 0.0208, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 224.7195, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 492.1620, train acc 0.998, test acc 0.500\n",
      "\n",
      "fraction =  0.8\n",
      "epoch 1, loss 4.3827, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 237.5205, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 357.2230, train acc 0.998, test acc 0.500\n",
      "\n",
      "fraction =  0.9\n",
      "epoch 1, loss 12.6584, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 265.6203, train acc 0.998, test acc 0.500\n",
      "epoch 3, loss 245.8700, train acc 0.999, test acc 0.500\n",
      "\n",
      "fraction =  0.95\n",
      "epoch 1, loss 0.0138, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 416.3008, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 330.0973, train acc 0.999, test acc 0.500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained = []\n",
    "num_epochs, lr = 3, 0.5\n",
    "loss = gloss.LogisticLoss()\n",
    "for i, datum in enumerate(splitted_data):\n",
    "    print(\"fraction = \", frac[i])\n",
    "    trained.append(train(datum, new_test, loss, num_epochs, 256, lr))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Shift Correction\n",
    "\n",
    "Having observed that covariate shift can be harmful, let's try fixing it. For this we first need to compute the appropriate propensity scores $\\frac{dp(x)}{dq(x)}$. For this purpose pick a biased dataset, let's say with $\\lambda = 0.1$ and try to fix the covariate shift.\n",
    "\n",
    "1. When training a logistic regression binary classifier to fix covariate shift, we assumed so far that both sets are of equal size. Show that re-weighting data in training and test set appropriately can help address the issue when both datasets have different size. What is the weighting?\n",
    "2. Train a binary classifier (using logistic regression) distinguishing between the biased training set and the unbiased test set. Note - you need to weigh the data. \n",
    "3. Use the scores to compute weights on the training set. Do they match the weight arising from the biasing distribution $\\lambda$? \n",
    "4. Train a binary classifier of the covariate shifted problem using the weights obtained previously and report the accuracy. Note - you will need to modify the training loop slightly such that you can compute the gradient of a weighted sum of losses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  \n",
    "The idea is that we want to estimate p(y|x) with labeled data(xi,yi). However, samples are drawn from q(x) rather than p(x). We can show that we can reweight the data to correct covariate shift.\n",
    "$$\n",
    "\\begin{array} { c } { \\int  p ( x ) f ( x )d x = \\int q ( x ) f ( x )\\frac { p ( x ) } { q ( x ) } d x = \\int  q ( x )  f ( x ) \\alpha ( x )d x} \\\\\n",
    "{ \\alpha ( x ) = \\frac { p ( x ) } { q ( x ) } = \\exp ( f ( x ) ) } \\end{array}\n",
    "$$\n",
    "the weighting is $$\n",
    "\\exp \\left( f \\left( x \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.976Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.4407, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 33.5446, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 35.8785, train acc 0.999, test acc 0.500\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "def train_with_weight(weight, train_iter, test_iter, loss, num_epochs, batch_size, lr=None):\n",
    "    net = getnet()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum() * weight\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (predict(y_hat) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = get_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "    return net\n",
    "\n",
    "biased_dataset = splitted_data[1]\n",
    "\n",
    "train_test_mix = []\n",
    "for i in range(len(biased_dataset)):\n",
    "    train_test_mix.append((biased_dataset[i][0].astype('float32').reshape(1, 784), nd.array([-1])))\n",
    "for i in range(len(new_test)):\n",
    "    train_test_mix.append((new_test[i][0].astype('float32').reshape(1, 784), nd.array([1])))\n",
    "    \n",
    "num_epochs, lr = 3, 0.05\n",
    "loss = gloss.LogisticLoss()\n",
    "result = train_with_weight(1/3, train_test_mix, new_test, loss, num_epochs, 256, lr) #2000/6000 = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we mixed biased traning dataset with unbiased test dataset. We got 99% accuracy on training dataset, which means that \n",
    "we can distinguish p(x) from q(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.984Z"
    }
   },
   "outputs": [],
   "source": [
    "#3\n",
    "def train_with_weight2(model, train_iter, test_iter, loss, num_epochs, batch_size, lr=None):\n",
    "    net = getnet()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "#                 print(\"exp(x)\", nd.exp(model(X)))\n",
    "#                 l = nd.exp(model(X)).sum().asscalar() * loss(y_hat, y).sum()\n",
    "                l = min(nd.exp(nd.sigmoid(model(X))).sum().asscalar(), 100) * loss(y_hat, y).sum()\n",
    "                #Above: we are reweighting by βi  = min(exp(f(xi)),100) to correct the covariate shift.\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (predict(y_hat) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = get_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T23:40:33.987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 28.3563, train acc 1.000, test acc 0.500\n",
      "epoch 2, loss 723.1970, train acc 0.999, test acc 0.500\n",
      "epoch 3, loss 1469.8216, train acc 0.998, test acc 0.500\n",
      "epoch 4, loss 1109.3656, train acc 0.998, test acc 0.500\n",
      "epoch 5, loss 2033.0729, train acc 0.997, test acc 0.500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dense(784 -> 1, linear)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "num_epochs, lr = 5, 0.5\n",
    "train_with_weight2(result, splitted_data[1], new_test, loss, num_epochs, 256, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
