{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 - Berkeley STAT 157\n",
    "\n",
    "**Your name: caojilin, SID 3033278367, teammates A,B,C (Please add your name, SID and teammates to ease Ryan and Rachel to grade.)**\n",
    "\n",
    "Handout 3/5/2019, due 3/12/2019 by 4pm. Please submit through gradescope.\n",
    "\n",
    "In this homework, we will train a CNN model on CIFAR-10 and submit the results into [Kaggle](https://www.kaggle.com/c/cifar-10). The rule is similar to homework 4: \n",
    "\n",
    "- work as a team\n",
    "- submit your results into Kaggle\n",
    "- take a screen shot of your best score and insert it below\n",
    "- the top 3 teams/individuals will be awarded with 500 dollar AWS credits\n",
    "\n",
    "The rest of this notebook contains a baseline ResNet-15 model to train on CIFAR-10. Please use it as a starting point. The end of this notebooks has several hints to improve your results.\n",
    "\n",
    "First, import the packages or modules required for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/sQq0XoE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import autograd, gluon, init\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain and Organize the Data Sets\n",
    "\n",
    "The competition data is divided into a training set and testing set. The training set contains 50,000 images. The testing set contains 300,000 images, of which 10,000 images are used for scoring, while the other 290,000 non-scoring images are included to prevent the manual labeling of the testing set and the submission of labeling results. The image formats in both data sets are PNG, with heights and widths of 32 pixels and three color channels (RGB). The images cover 10 categories: planes, cars, birds, cats, deer, dogs, frogs, horses, boats, and trucks. The upper-left corner of Figure 9.16 shows some images of planes, cars, and birds in the data set.\n",
    "\n",
    "### Download the Data Set\n",
    "\n",
    "After logging in to Kaggle, we can click on the \"Data\" tab on the CIFAR-10 image classification competition webpage shown in Figure 9.16 and download the training data set \"train.7z\", the testing data set \"test.7z\", and the training data set labels \"trainlabels.csv\".\n",
    "\n",
    "\n",
    "### Unzip the Data Set\n",
    "\n",
    "The training data set \"train.7z\" and the test data set \"test.7z\" need to be unzipped after downloading. After unzipping the data sets, store the training data set, test data set, and training data set labels in the following respective paths:\n",
    "\n",
    "* ../data/kaggle_cifar10/train/[1-50000].png\n",
    "* ../data/kaggle_cifar10/test/[1-300000].png\n",
    "* ../data/kaggle_cifar10/trainLabels.csv\n",
    "\n",
    "To make it easier to get started, we provide a small-scale sample of the data set mentioned above. \"train_tiny.zip\" contains 100 training examples, while \"test_tiny.zip\" contains only one test example. Their unzipped folder names are \"train_tiny\" and \"test_tiny\", respectively. In addition, unzip the zip file of the training data set labels to obtain the file \"trainlabels.csv\". If you are going to use the full data set of the Kaggle competition, you will also need to change the following `demo` variable to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "demo = True # You need to change demo to False for this homework. \n",
    "if demo:\n",
    "    import zipfile\n",
    "    for f in ['train_tiny.zip', 'test_tiny.zip', 'trainLabels.csv.zip']:\n",
    "        with zipfile.ZipFile('../data/kaggle_cifar10/' + f, 'r') as z:\n",
    "            z.extractall('../data/kaggle_cifar10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the Data Set\n",
    "\n",
    "We need to organize data sets to facilitate model training and testing. The following `read_label_file` function will be used to read the label file for the training data set. The parameter `valid_ratio` in this function is the ratio of the number of examples in the validation set to the number of examples in the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "def read_label_file(data_dir, label_file, train_dir, valid_ratio):\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # Skip the file header line (column name)\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((int(idx), label) for idx, label in tokens))\n",
    "    labels = set(idx_label.values())\n",
    "    n_train_valid = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    n_train = int(n_train_valid * (1 - valid_ratio))\n",
    "    assert 0 < n_train < n_train_valid\n",
    "    return n_train // len(labels), idx_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a helper function to create a path only if the path does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the `reorg_train_valid` function to segment the validation set from the original training set.  Here, we use `valid_ratio=0.1` as an example. Since the original training set has 50,000 images, there will be 45,000 images used for training and stored in the path “`input_dir/train`” when tuning hyper-parameters, while the other 5,000 images will be stored as validation set in the path “`input_dir/valid`”. After organizing the data, images of the same type will be placed under the same folder so that we can read them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "def reorg_train_valid(data_dir, train_dir, input_dir, n_train_per_label,\n",
    "                      idx_label):\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = int(train_file.split('.')[0])\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < n_train_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reorg_test` function below is used to organize the testing set to facilitate the reading during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "def reorg_test(data_dir, test_dir, input_dir):\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a function to call the previously defined `reorg_test`, `reorg_train_valid`, and `reorg_test` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "def reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                       valid_ratio):\n",
    "    n_train_per_label, idx_label = read_label_file(data_dir, label_file,\n",
    "                                                   train_dir, valid_ratio)\n",
    "    reorg_train_valid(data_dir, train_dir, input_dir, n_train_per_label,\n",
    "                      idx_label)\n",
    "    reorg_test(data_dir, test_dir, input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use only 100 training example and one test example here. The folder names for the training and testing data sets are \"train_tiny\" and \"test_tiny\", respectively. Accordingly, we only set the batch size to 1. During actual training and testing, the complete data set of the Kaggle competition should be used and `batch_size` should be set to a larger integer, such as 128. We use 10% of the training examples as the validation set for tuning hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "if demo:\n",
    "    # Note: Here, we use small training sets and small testing sets and the\n",
    "    # batch size should be set smaller. When using the complete data set for\n",
    "    # the Kaggle competition, the batch size can be set to a large integer\n",
    "    train_dir, test_dir, batch_size = 'train_tiny', 'test_tiny', 1\n",
    "else:\n",
    "    train_dir, test_dir, batch_size = 'train', 'test', 64\n",
    "data_dir, label_file = '../data/kaggle_cifar10', 'trainLabels.csv'\n",
    "input_dir, valid_ratio = 'train_valid_test', 0.1\n",
    "reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "(We will cover image augmentation next week, you can ignore it for this homework.)\n",
    "\n",
    "To cope with overfitting, we use image augmentation. For example, by adding `transforms.RandomFlipLeftRight()`, the images can be flipped at random. We can also perform normalization for the three RGB channels of color images using `transforms.Normalize()`. Below, we list some of these operations that you can choose to use or modify depending on requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = gdata.vision.transforms.Compose([\n",
    "    # Magnify the image to a square of 40 pixels in both height and width\n",
    "    gdata.vision.transforms.Resize(40),\n",
    "    # Randomly crop a square image of 40 pixels in both height and width to\n",
    "    # produce a small square of 0.64 to 1 times the area of the original\n",
    "    # image, and then shrink it to a square of 32 pixels in both height and\n",
    "    # width\n",
    "    gdata.vision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "                                              ratio=(1.0, 1.0)),\n",
    "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    # Normalize each channel of the image\n",
    "    gdata.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                      [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure the certainty of the output during testing, we only perform normalization on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    gdata.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                      [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data Set\n",
    "\n",
    "Next, we can create the `ImageFolderDataset` instance to read the organized data set containing the original image files, where each data instance includes the image and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "# Read the original image file. Flag=1 indicates that the input image has\n",
    "# three channels (color)\n",
    "train_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'train'), flag=1)\n",
    "valid_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'valid'), flag=1)\n",
    "train_valid_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'train_valid'), flag=1)\n",
    "test_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'test'), flag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the defined image augmentation operation in `DataLoader`. During training, we only use the validation set to evaluate the model, so we need to ensure the certainty of the output. During prediction, we will train the model on the combined training set and validation set to make full use of all labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = gdata.DataLoader(train_ds.transform_first(transform_train),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "valid_iter = gdata.DataLoader(valid_ds.transform_first(transform_test),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_iter = gdata.DataLoader(train_valid_ds.transform_first(\n",
    "    transform_train), batch_size, shuffle=True, last_batch='keep')\n",
    "test_iter = gdata.DataLoader(test_ds.transform_first(transform_test),\n",
    "                             batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "(We will cover hybridize next week. It often makes your model run faster, but you can ignore what it means for this homework.)\n",
    "\n",
    "Here, we build the residual blocks based on the HybridBlock class, which is slightly different than the implementation described in the [“Residual networks (ResNet)”](http://d2l.ai/chapter_convolutional-neural-networks/resnet.html) section. This is done to improve execution efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n",
    "                               strides=strides)\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\n",
    "                                   strides=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "\n",
    "    def hybrid_forward(self, F, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.relu(Y + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the ResNet-18 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes):\n",
    "    net = nn.HybridSequential()\n",
    "    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n",
    "            nn.BatchNorm(), nn.Activation('relu'))\n",
    "\n",
    "    def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "        blk = nn.HybridSequential()\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.add(Residual(num_channels))\n",
    "        return blk\n",
    "\n",
    "    net.add(resnet_block(64, 2, first_block=True),\n",
    "            resnet_block(128, 2),\n",
    "            resnet_block(256, 2),\n",
    "            resnet_block(512, 2))\n",
    "    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_v2_bottleneck(nn.HybridBlock):\n",
    "    \n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual_v2_bottleneck, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv1 = nn.Conv2D(channels=channels//4, kernel_size=1, use_bias=False)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels=channels//4, kernel_size=3, strides=strides, padding=1, use_bias=False)\n",
    "            self.bn3 = nn.BatchNorm()\n",
    "            self.conv3 = nn.Conv2D(channels=channels, kernel_size=1, use_bias=False)\n",
    "            self.bn4 = nn.BatchNorm()   # add this\n",
    "            \n",
    "            if not same_shape:\n",
    "                self.conv4 = nn.Conv2D(channels=channels, kernel_size=1, strides=strides, use_bias=False)\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = self.conv1(self.bn1(x))  # remove relu\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out = self.bn4(out)            # add this\n",
    "        \n",
    "        if not self.same_shape:\n",
    "            x = self.conv4(x)\n",
    "        return out + x\n",
    "    \n",
    "\n",
    "class ResNet164_v2(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet164_v2, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=64, kernel_size=3, padding=1, strides=1, use_bias=False))\n",
    "            # block 2\n",
    "            for _ in range(27):\n",
    "                net.add(Residual_v2_bottleneck(channels=64))\n",
    "            # block 3\n",
    "            net.add(Residual_v2_bottleneck(128, same_shape=False))\n",
    "            for _ in range(26):\n",
    "                net.add(Residual_v2_bottleneck(channels=128))\n",
    "            # block4\n",
    "            net.add(Residual_v2_bottleneck(256, same_shape=False))\n",
    "            for _ in range(26):\n",
    "                net.add(Residual_v2_bottleneck(channels=256))\n",
    "            # block 5\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes)) \n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print \"Block %d output %s\" % (i+1, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 image classification challenge uses 10 categories. We will perform Xavier random initialization on the model before training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(ctx):\n",
    "    num_classes = 10\n",
    "    net = ResNet164_v2(num_classes)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Functions\n",
    "\n",
    "We will select the model and tune hyper-parameters according to the model's performance on the validation set. Next, we define the model training function `train`. We record the training time of each epoch, which helps us compare the time costs of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in train_iter:\n",
    "            y = y.astype('float32').as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X.as_in_context(ctx))\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        time_s = \"time %.2f sec\" % (time.time() - start)\n",
    "        if valid_iter is not None:\n",
    "            valid_acc = d2l.evaluate_accuracy(valid_iter, net, ctx)\n",
    "            epoch_s = (\"epoch %d, loss %f, train acc %f, valid acc %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n, train_acc_sum / n,\n",
    "                       valid_acc))\n",
    "        else:\n",
    "            epoch_s = (\"epoch %d, loss %f, train acc %f, \" %\n",
    "                       (epoch + 1, train_l_sum / n, train_acc_sum / n))\n",
    "        print(epoch_s + time_s + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate the Model\n",
    "\n",
    "Now, we can train and validate the model. The following hyper-parameters can be tuned. For example, we can increase the number of epochs. Because `lr_period` and `lr_decay` are set to 80 and 0.1 respectively, the learning rate of the optimization algorithm will be multiplied by 0.1 after every 80 epochs. For simplicity, we only train one epoch here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = gdata.DataLoader(train_ds.transform_first(transform_train),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "valid_iter = gdata.DataLoader(valid_ds.transform_first(transform_test),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_iter = gdata.DataLoader(train_valid_ds.transform_first(\n",
    "    transform_train), batch_size, shuffle=True, last_batch='keep')\n",
    "test_iter = gdata.DataLoader(test_ds.transform_first(transform_test),\n",
    "                             batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.16593, train_acc 0.3018, valid_acc 0.3185, Time 00:02:24,lr 0.1\n",
      "epoch 1, loss 1.97320, train_acc 0.3922, valid_acc 0.3997, Time 00:02:43,lr 0.1\n",
      "epoch 2, loss 1.87724, train_acc 0.4802, valid_acc 0.4904, Time 00:02:46,lr 0.1\n",
      "epoch 3, loss 1.77747, train_acc 0.5290, valid_acc 0.5330, Time 00:02:51,lr 0.1\n",
      "epoch 4, loss 1.70237, train_acc 0.6065, valid_acc 0.6088, Time 00:02:49,lr 0.1\n",
      "epoch 5, loss 1.64160, train_acc 0.6250, valid_acc 0.6280, Time 00:02:50,lr 0.1\n",
      "epoch 6, loss 1.59686, train_acc 0.6751, valid_acc 0.6758, Time 00:02:49,lr 0.1\n",
      "epoch 7, loss 1.56121, train_acc 0.6758, valid_acc 0.6765, Time 00:02:50,lr 0.1\n",
      "epoch 8, loss 1.52487, train_acc 0.6424, valid_acc 0.6397, Time 00:02:50,lr 0.1\n",
      "epoch 9, loss 1.49709, train_acc 0.6987, valid_acc 0.6961, Time 00:02:50,lr 0.1\n",
      "epoch 10, loss 1.50484, train_acc 0.7280, valid_acc 0.7212, Time 00:02:46,lr 0.1\n",
      "epoch 11, loss 1.47519, train_acc 0.6725, valid_acc 0.6656, Time 00:02:48,lr 0.1\n",
      "epoch 12, loss 1.45798, train_acc 0.7600, valid_acc 0.7484, Time 00:02:52,lr 0.1\n",
      "epoch 13, loss 1.44291, train_acc 0.6156, valid_acc 0.5938, Time 00:02:51,lr 0.1\n",
      "epoch 14, loss 1.45762, train_acc 0.7605, valid_acc 0.7583, Time 00:02:51,lr 0.1\n",
      "epoch 15, loss 1.43096, train_acc 0.7232, valid_acc 0.7098, Time 00:02:51,lr 0.1\n",
      "epoch 16, loss 1.41539, train_acc 0.7631, valid_acc 0.7487, Time 00:02:53,lr 0.1\n",
      "epoch 17, loss 1.41253, train_acc 0.7540, valid_acc 0.7345, Time 00:02:51,lr 0.1\n",
      "epoch 18, loss 1.40944, train_acc 0.7716, valid_acc 0.7644, Time 00:02:49,lr 0.1\n",
      "epoch 19, loss 1.40919, train_acc 0.7287, valid_acc 0.7108, Time 00:02:51,lr 0.1\n",
      "epoch 20, loss 1.39907, train_acc 0.7800, valid_acc 0.7620, Time 00:02:51,lr 0.1\n",
      "epoch 21, loss 1.39861, train_acc 0.7807, valid_acc 0.7715, Time 00:02:52,lr 0.1\n",
      "epoch 22, loss 1.39499, train_acc 0.7514, valid_acc 0.7349, Time 00:02:51,lr 0.1\n",
      "epoch 23, loss 1.37361, train_acc 0.7128, valid_acc 0.6935, Time 00:02:55,lr 0.1\n",
      "epoch 24, loss 1.37417, train_acc 0.7443, valid_acc 0.7253, Time 00:02:53,lr 0.1\n",
      "epoch 25, loss 1.37380, train_acc 0.7866, valid_acc 0.7695, Time 00:02:43,lr 0.1\n",
      "epoch 26, loss 1.37925, train_acc 0.7879, valid_acc 0.7765, Time 00:02:46,lr 0.1\n",
      "epoch 27, loss 1.37340, train_acc 0.7796, valid_acc 0.7598, Time 00:03:16,lr 0.1\n",
      "epoch 28, loss 1.37557, train_acc 0.7398, valid_acc 0.7125, Time 00:03:10,lr 0.1\n",
      "epoch 29, loss 1.36707, train_acc 0.7860, valid_acc 0.7735, Time 00:03:15,lr 0.1\n",
      "epoch 30, loss 1.36420, train_acc 0.7993, valid_acc 0.7810, Time 00:03:23,lr 0.1\n",
      "epoch 31, loss 1.35215, train_acc 0.8120, valid_acc 0.7932, Time 00:03:17,lr 0.1\n",
      "epoch 32, loss 1.35631, train_acc 0.7857, valid_acc 0.7702, Time 00:03:14,lr 0.1\n",
      "epoch 33, loss 1.35603, train_acc 0.7815, valid_acc 0.7667, Time 00:03:12,lr 0.1\n",
      "epoch 34, loss 1.34795, train_acc 0.7454, valid_acc 0.7367, Time 00:03:10,lr 0.1\n",
      "epoch 35, loss 1.34180, train_acc 0.8172, valid_acc 0.8031, Time 00:03:14,lr 0.1\n",
      "epoch 36, loss 1.35993, train_acc 0.7833, valid_acc 0.7733, Time 00:03:12,lr 0.1\n",
      "epoch 37, loss 1.33974, train_acc 0.7398, valid_acc 0.7188, Time 00:03:15,lr 0.1\n",
      "epoch 38, loss 1.36017, train_acc 0.7912, valid_acc 0.7699, Time 00:03:15,lr 0.1\n",
      "epoch 39, loss 1.34915, train_acc 0.7875, valid_acc 0.7749, Time 00:03:05,lr 0.1\n",
      "epoch 40, loss 1.32973, train_acc 0.7747, valid_acc 0.7594, Time 00:03:01,lr 0.1\n",
      "epoch 41, loss 1.33626, train_acc 0.7625, valid_acc 0.7478, Time 00:03:03,lr 0.1\n",
      "epoch 42, loss 1.34808, train_acc 0.7849, valid_acc 0.7636, Time 00:03:03,lr 0.1\n",
      "epoch 43, loss 1.34154, train_acc 0.8158, valid_acc 0.8040, Time 00:03:04,lr 0.1\n",
      "epoch 44, loss 1.33321, train_acc 0.8199, valid_acc 0.8063, Time 00:03:04,lr 0.1\n",
      "epoch 45, loss 1.33215, train_acc 0.7801, valid_acc 0.7579, Time 00:03:04,lr 0.1\n",
      "epoch 46, loss 1.33544, train_acc 0.7546, valid_acc 0.7470, Time 00:03:07,lr 0.1\n",
      "epoch 47, loss 1.34467, train_acc 0.7803, valid_acc 0.7600, Time 00:03:03,lr 0.1\n",
      "epoch 48, loss 1.32607, train_acc 0.8112, valid_acc 0.7971, Time 00:03:03,lr 0.1\n",
      "epoch 49, loss 1.33663, train_acc 0.7947, valid_acc 0.7833, Time 00:03:05,lr 0.1\n",
      "epoch 50, loss 1.34501, train_acc 0.7905, valid_acc 0.7728, Time 00:03:07,lr 0.1\n",
      "epoch 51, loss 1.32826, train_acc 0.7878, valid_acc 0.7675, Time 00:03:05,lr 0.1\n",
      "epoch 52, loss 1.32729, train_acc 0.7810, valid_acc 0.7703, Time 00:03:05,lr 0.1\n",
      "epoch 53, loss 1.32485, train_acc 0.8157, valid_acc 0.8082, Time 00:03:11,lr 0.1\n",
      "epoch 54, loss 1.33577, train_acc 0.7834, valid_acc 0.7568, Time 00:03:04,lr 0.1\n",
      "epoch 55, loss 1.32720, train_acc 0.7768, valid_acc 0.7620, Time 00:03:05,lr 0.1\n",
      "epoch 56, loss 1.33960, train_acc 0.8235, valid_acc 0.8161, Time 00:05:52,lr 0.1\n",
      "epoch 57, loss 1.31656, train_acc 0.7968, valid_acc 0.7894, Time 00:13:25,lr 0.1\n",
      "epoch 58, loss 1.31316, train_acc 0.8177, valid_acc 0.8108, Time 00:17:19,lr 0.1\n",
      "epoch 59, loss 1.33070, train_acc 0.8314, valid_acc 0.8126, Time 00:10:16,lr 0.1\n",
      "epoch 60, loss 1.32717, train_acc 0.7980, valid_acc 0.7887, Time 00:03:17,lr 0.1\n",
      "epoch 61, loss 1.32127, train_acc 0.8070, valid_acc 0.7889, Time 00:03:09,lr 0.1\n",
      "epoch 62, loss 1.32567, train_acc 0.7961, valid_acc 0.7795, Time 00:03:10,lr 0.1\n",
      "epoch 63, loss 1.33797, train_acc 0.8074, valid_acc 0.8001, Time 00:03:04,lr 0.1\n",
      "epoch 64, loss 1.29923, train_acc 0.7971, valid_acc 0.7815, Time 00:03:12,lr 0.1\n",
      "epoch 65, loss 1.31300, train_acc 0.8078, valid_acc 0.7979, Time 00:03:34,lr 0.1\n",
      "epoch 66, loss 1.32401, train_acc 0.7414, valid_acc 0.7237, Time 00:03:22,lr 0.1\n",
      "epoch 67, loss 1.31541, train_acc 0.8264, valid_acc 0.8114, Time 00:03:07,lr 0.1\n",
      "epoch 68, loss 1.32304, train_acc 0.8025, valid_acc 0.7884, Time 00:02:59,lr 0.1\n",
      "epoch 69, loss 1.33490, train_acc 0.7639, valid_acc 0.7398, Time 00:02:57,lr 0.1\n",
      "epoch 70, loss 1.34050, train_acc 0.8351, valid_acc 0.8208, Time 00:03:01,lr 0.1\n",
      "epoch 71, loss 1.31406, train_acc 0.7746, valid_acc 0.7570, Time 00:03:02,lr 0.1\n",
      "epoch 72, loss 1.29430, train_acc 0.8064, valid_acc 0.7842, Time 00:03:05,lr 0.1\n",
      "epoch 73, loss 1.31676, train_acc 0.8037, valid_acc 0.7839, Time 00:03:06,lr 0.1\n",
      "epoch 74, loss 1.31748, train_acc 0.7416, valid_acc 0.7223, Time 00:04:37,lr 0.1\n",
      "epoch 75, loss 1.30156, train_acc 0.7944, valid_acc 0.7782, Time 00:17:31,lr 0.1\n",
      "epoch 76, loss 1.32197, train_acc 0.7859, valid_acc 0.7640, Time 00:17:31,lr 0.1\n",
      "epoch 77, loss 1.32253, train_acc 0.8163, valid_acc 0.8001, Time 00:07:10,lr 0.1\n",
      "epoch 78, loss 1.33143, train_acc 0.7925, valid_acc 0.7768, Time 00:03:26,lr 0.1\n",
      "epoch 79, loss 1.31097, train_acc 0.8077, valid_acc 0.7898, Time 00:03:04,lr 0.1\n",
      "epoch 80, loss 1.20304, train_acc 0.8780, valid_acc 0.8592, Time 00:03:19,lr 0.01\n",
      "epoch 81, loss 1.14880, train_acc 0.8944, valid_acc 0.8764, Time 00:03:14,lr 0.01\n",
      "epoch 82, loss 1.14279, train_acc 0.8997, valid_acc 0.8846, Time 00:03:06,lr 0.01\n",
      "epoch 83, loss 1.12559, train_acc 0.8999, valid_acc 0.8835, Time 00:03:19,lr 0.01\n",
      "epoch 84, loss 1.14183, train_acc 0.9056, valid_acc 0.8877, Time 00:03:07,lr 0.01\n",
      "epoch 85, loss 1.10517, train_acc 0.9109, valid_acc 0.8975, Time 00:03:09,lr 0.01\n",
      "epoch 86, loss 1.12380, train_acc 0.9147, valid_acc 0.8973, Time 00:03:07,lr 0.01\n",
      "epoch 87, loss 1.10470, train_acc 0.9093, valid_acc 0.8908, Time 00:03:09,lr 0.01\n",
      "epoch 88, loss 1.09398, train_acc 0.9128, valid_acc 0.8927, Time 00:03:09,lr 0.01\n",
      "epoch 89, loss 1.09117, train_acc 0.9117, valid_acc 0.8899, Time 00:03:11,lr 0.01\n",
      "epoch 90, loss 1.09854, train_acc 0.9201, valid_acc 0.9009, Time 00:03:06,lr 0.01\n",
      "epoch 91, loss 1.09604, train_acc 0.9193, valid_acc 0.8959, Time 00:03:06,lr 0.01\n",
      "epoch 92, loss 1.07482, train_acc 0.9182, valid_acc 0.8973, Time 00:03:15,lr 0.01\n",
      "epoch 93, loss 1.09745, train_acc 0.9165, valid_acc 0.8902, Time 00:06:27,lr 0.01\n",
      "epoch 94, loss 1.07607, train_acc 0.9263, valid_acc 0.9030, Time 00:17:37,lr 0.01\n",
      "epoch 95, loss 1.07369, train_acc 0.9214, valid_acc 0.8996, Time 00:17:45,lr 0.01\n",
      "epoch 96, loss 1.07177, train_acc 0.9225, valid_acc 0.8964, Time 00:05:32,lr 0.01\n",
      "epoch 97, loss 1.06274, train_acc 0.9271, valid_acc 0.8991, Time 00:03:19,lr 0.01\n",
      "epoch 98, loss 1.09669, train_acc 0.9258, valid_acc 0.8999, Time 00:03:18,lr 0.01\n",
      "epoch 99, loss 1.07988, train_acc 0.9306, valid_acc 0.9061, Time 00:06:53,lr 0.01\n",
      "epoch 100, loss 1.08015, train_acc 0.9270, valid_acc 0.9012, Time 00:17:35,lr 0.01\n",
      "epoch 101, loss 1.08203, train_acc 0.9295, valid_acc 0.9018, Time 00:17:38,lr 0.01\n",
      "epoch 102, loss 1.08058, train_acc 0.9311, valid_acc 0.9021, Time 00:05:31,lr 0.01\n",
      "epoch 103, loss 1.08665, train_acc 0.9260, valid_acc 0.9001, Time 00:03:25,lr 0.01\n",
      "epoch 104, loss 1.08120, train_acc 0.9300, valid_acc 0.9021, Time 00:03:10,lr 0.01\n",
      "epoch 105, loss 1.05957, train_acc 0.9265, valid_acc 0.8975, Time 00:03:10,lr 0.01\n",
      "epoch 106, loss 1.05478, train_acc 0.9266, valid_acc 0.8982, Time 00:03:13,lr 0.01\n",
      "epoch 107, loss 1.06689, train_acc 0.9312, valid_acc 0.9001, Time 00:03:11,lr 0.01\n",
      "epoch 108, loss 1.06822, train_acc 0.9312, valid_acc 0.9061, Time 00:03:13,lr 0.01\n",
      "epoch 109, loss 1.06758, train_acc 0.9385, valid_acc 0.9105, Time 00:03:14,lr 0.01\n",
      "epoch 110, loss 1.07110, train_acc 0.9270, valid_acc 0.8968, Time 00:03:13,lr 0.01\n",
      "epoch 111, loss 1.06828, train_acc 0.9307, valid_acc 0.8983, Time 00:03:13,lr 0.01\n",
      "epoch 112, loss 1.05636, train_acc 0.9328, valid_acc 0.9036, Time 00:03:21,lr 0.01\n",
      "epoch 113, loss 1.07447, train_acc 0.9359, valid_acc 0.9053, Time 00:04:34,lr 0.01\n",
      "epoch 114, loss 1.07265, train_acc 0.9332, valid_acc 0.9059, Time 00:17:39,lr 0.01\n",
      "epoch 115, loss 1.05529, train_acc 0.9348, valid_acc 0.9028, Time 00:17:39,lr 0.01\n",
      "epoch 116, loss 1.05023, train_acc 0.9256, valid_acc 0.8970, Time 00:07:46,lr 0.01\n",
      "epoch 117, loss 1.04672, train_acc 0.9373, valid_acc 0.9057, Time 00:03:22,lr 0.01\n",
      "epoch 118, loss 1.03829, train_acc 0.9320, valid_acc 0.9025, Time 00:03:32,lr 0.01\n",
      "epoch 119, loss 1.08419, train_acc 0.9336, valid_acc 0.9012, Time 00:03:19,lr 0.01\n",
      "epoch 120, loss 1.05413, train_acc 0.9351, valid_acc 0.9056, Time 00:14:12,lr 0.01\n",
      "epoch 121, loss 1.04569, train_acc 0.9354, valid_acc 0.9079, Time 00:17:39,lr 0.01\n",
      "epoch 122, loss 1.07001, train_acc 0.9415, valid_acc 0.9089, Time 00:12:30,lr 0.01\n",
      "epoch 123, loss 1.07387, train_acc 0.9355, valid_acc 0.9066, Time 00:03:40,lr 0.01\n",
      "epoch 124, loss 1.05514, train_acc 0.9285, valid_acc 0.8993, Time 00:03:26,lr 0.01\n",
      "epoch 125, loss 1.04990, train_acc 0.9395, valid_acc 0.9066, Time 00:03:27,lr 0.01\n",
      "epoch 126, loss 1.05501, train_acc 0.9334, valid_acc 0.9089, Time 00:03:22,lr 0.01\n",
      "epoch 127, loss 1.06621, train_acc 0.9315, valid_acc 0.9032, Time 00:12:21,lr 0.01\n",
      "epoch 128, loss 1.04811, train_acc 0.9385, valid_acc 0.9048, Time 00:18:00,lr 0.01\n",
      "epoch 129, loss 1.05576, train_acc 0.9301, valid_acc 0.8980, Time 00:13:53,lr 0.01\n",
      "epoch 130, loss 1.07707, train_acc 0.9334, valid_acc 0.9005, Time 00:04:08,lr 0.01\n",
      "epoch 131, loss 1.03549, train_acc 0.9359, valid_acc 0.9057, Time 00:08:16,lr 0.01\n",
      "epoch 132, loss 1.04940, train_acc 0.9345, valid_acc 0.9013, Time 00:17:44,lr 0.01\n",
      "epoch 133, loss 1.05732, train_acc 0.9322, valid_acc 0.9077, Time 00:14:53,lr 0.01\n",
      "epoch 134, loss 1.06246, train_acc 0.9346, valid_acc 0.9062, Time 00:03:42,lr 0.01\n",
      "epoch 135, loss 1.05425, train_acc 0.9290, valid_acc 0.8943, Time 00:03:27,lr 0.01\n",
      "epoch 136, loss 1.04927, train_acc 0.9318, valid_acc 0.8996, Time 00:03:25,lr 0.01\n",
      "epoch 137, loss 1.03976, train_acc 0.9398, valid_acc 0.9097, Time 00:03:30,lr 0.01\n",
      "epoch 138, loss 1.04665, train_acc 0.9309, valid_acc 0.8946, Time 00:03:27,lr 0.01\n",
      "epoch 139, loss 1.04799, train_acc 0.9379, valid_acc 0.9074, Time 00:03:24,lr 0.01\n",
      "epoch 140, loss 1.05712, train_acc 0.9353, valid_acc 0.9042, Time 00:03:28,lr 0.01\n",
      "epoch 141, loss 1.05111, train_acc 0.9382, valid_acc 0.9080, Time 00:03:27,lr 0.01\n",
      "epoch 142, loss 1.03855, train_acc 0.9345, valid_acc 0.9018, Time 00:03:25,lr 0.01\n",
      "epoch 143, loss 1.02854, train_acc 0.9428, valid_acc 0.9129, Time 00:03:27,lr 0.01\n",
      "epoch 144, loss 1.04087, train_acc 0.9386, valid_acc 0.9081, Time 00:03:19,lr 0.01\n",
      "epoch 145, loss 1.03890, train_acc 0.9356, valid_acc 0.9010, Time 00:03:22,lr 0.01\n",
      "epoch 146, loss 1.03986, train_acc 0.9304, valid_acc 0.9002, Time 00:03:26,lr 0.01\n",
      "epoch 147, loss 1.04848, train_acc 0.9377, valid_acc 0.9056, Time 00:03:32,lr 0.01\n",
      "epoch 148, loss 1.04825, train_acc 0.9367, valid_acc 0.9075, Time 00:03:29,lr 0.01\n",
      "epoch 149, loss 1.04058, train_acc 0.9272, valid_acc 0.8968, Time 00:03:26,lr 0.01\n",
      "epoch 150, loss 1.01519, train_acc 0.9564, valid_acc 0.9250, Time 00:03:20,lr 0.001\n",
      "epoch 151, loss 1.00549, train_acc 0.9589, valid_acc 0.9281, Time 00:03:25,lr 0.001\n",
      "epoch 152, loss 0.97932, train_acc 0.9622, valid_acc 0.9313, Time 00:03:29,lr 0.001\n",
      "epoch 153, loss 0.98294, train_acc 0.9609, valid_acc 0.9287, Time 00:03:16,lr 0.001\n",
      "epoch 154, loss 0.97573, train_acc 0.9608, valid_acc 0.9279, Time 00:03:12,lr 0.001\n",
      "epoch 155, loss 0.95079, train_acc 0.9650, valid_acc 0.9313, Time 00:03:08,lr 0.001\n",
      "epoch 156, loss 0.95617, train_acc 0.9647, valid_acc 0.9294, Time 00:03:09,lr 0.001\n",
      "epoch 157, loss 0.96072, train_acc 0.9658, valid_acc 0.9312, Time 00:03:08,lr 0.001\n",
      "epoch 158, loss 0.96697, train_acc 0.9661, valid_acc 0.9323, Time 00:03:10,lr 0.001\n",
      "epoch 159, loss 0.96537, train_acc 0.9680, valid_acc 0.9333, Time 00:03:07,lr 0.001\n",
      "epoch 160, loss 0.97062, train_acc 0.9683, valid_acc 0.9312, Time 00:03:10,lr 0.001\n",
      "epoch 161, loss 0.95298, train_acc 0.9687, valid_acc 0.9325, Time 00:03:08,lr 0.001\n",
      "epoch 162, loss 0.96177, train_acc 0.9658, valid_acc 0.9306, Time 00:03:09,lr 0.001\n",
      "epoch 163, loss 0.95081, train_acc 0.9690, valid_acc 0.9339, Time 00:03:09,lr 0.001\n",
      "epoch 164, loss 0.93594, train_acc 0.9696, valid_acc 0.9326, Time 00:03:11,lr 0.001\n",
      "epoch 165, loss 0.95492, train_acc 0.9687, valid_acc 0.9345, Time 00:03:09,lr 0.001\n",
      "epoch 166, loss 0.97178, train_acc 0.9704, valid_acc 0.9322, Time 00:03:08,lr 0.001\n",
      "epoch 167, loss 0.95078, train_acc 0.9702, valid_acc 0.9324, Time 00:03:10,lr 0.001\n",
      "epoch 168, loss 0.96071, train_acc 0.9689, valid_acc 0.9326, Time 00:03:10,lr 0.001\n",
      "epoch 169, loss 0.93886, train_acc 0.9710, valid_acc 0.9342, Time 00:03:10,lr 0.001\n",
      "epoch 170, loss 0.94743, train_acc 0.9728, valid_acc 0.9334, Time 00:03:11,lr 0.001\n",
      "epoch 171, loss 0.94780, train_acc 0.9717, valid_acc 0.9340, Time 00:03:12,lr 0.001\n",
      "epoch 172, loss 0.95980, train_acc 0.9689, valid_acc 0.9309, Time 00:03:09,lr 0.001\n",
      "epoch 173, loss 0.94095, train_acc 0.9727, valid_acc 0.9347, Time 00:03:09,lr 0.001\n",
      "epoch 174, loss 0.92987, train_acc 0.9736, valid_acc 0.9359, Time 00:03:09,lr 0.001\n",
      "epoch 175, loss 0.95549, train_acc 0.9696, valid_acc 0.9308, Time 00:03:10,lr 0.001\n",
      "epoch 176, loss 0.95595, train_acc 0.9726, valid_acc 0.9326, Time 00:03:09,lr 0.001\n",
      "epoch 177, loss 0.92947, train_acc 0.9738, valid_acc 0.9339, Time 00:03:10,lr 0.001\n",
      "epoch 178, loss 0.95370, train_acc 0.9723, valid_acc 0.9349, Time 00:03:08,lr 0.001\n",
      "epoch 179, loss 0.93811, train_acc 0.9701, valid_acc 0.9328, Time 00:03:09,lr 0.001\n",
      "epoch 180, loss 0.95937, train_acc 0.9726, valid_acc 0.9340, Time 00:03:11,lr 0.001\n",
      "epoch 181, loss 0.93950, train_acc 0.9752, valid_acc 0.9367, Time 00:03:10,lr 0.001\n",
      "epoch 182, loss 0.94722, train_acc 0.9721, valid_acc 0.9335, Time 00:03:12,lr 0.001\n",
      "epoch 183, loss 0.93646, train_acc 0.9719, valid_acc 0.9325, Time 00:03:10,lr 0.001\n",
      "epoch 184, loss 0.92570, train_acc 0.9739, valid_acc 0.9344, Time 00:03:13,lr 0.001\n",
      "epoch 185, loss 0.93027, train_acc 0.9747, valid_acc 0.9343, Time 00:03:13,lr 0.001\n",
      "epoch 186, loss 0.94062, train_acc 0.9722, valid_acc 0.9315, Time 00:03:12,lr 0.001\n",
      "epoch 187, loss 0.92805, train_acc 0.9750, valid_acc 0.9347, Time 00:03:10,lr 0.001\n",
      "epoch 188, loss 0.94461, train_acc 0.9739, valid_acc 0.9324, Time 00:03:10,lr 0.001\n",
      "epoch 189, loss 0.94548, train_acc 0.9754, valid_acc 0.9353, Time 00:03:15,lr 0.001\n",
      "epoch 190, loss 0.95523, train_acc 0.9763, valid_acc 0.9357, Time 00:03:14,lr 0.001\n",
      "epoch 191, loss 0.93185, train_acc 0.9758, valid_acc 0.9360, Time 00:03:11,lr 0.001\n",
      "epoch 192, loss 0.92303, train_acc 0.9740, valid_acc 0.9319, Time 00:03:13,lr 0.001\n",
      "epoch 193, loss 0.93946, train_acc 0.9757, valid_acc 0.9315, Time 00:03:13,lr 0.001\n",
      "epoch 194, loss 0.92639, train_acc 0.9743, valid_acc 0.9313, Time 00:03:14,lr 0.001\n",
      "epoch 195, loss 0.94102, train_acc 0.9752, valid_acc 0.9319, Time 00:03:13,lr 0.001\n",
      "epoch 196, loss 0.91991, train_acc 0.9761, valid_acc 0.9337, Time 00:03:13,lr 0.001\n",
      "epoch 197, loss 0.94033, train_acc 0.9772, valid_acc 0.9347, Time 00:03:12,lr 0.001\n",
      "epoch 198, loss 0.94253, train_acc 0.9786, valid_acc 0.9363, Time 00:03:12,lr 0.001\n",
      "epoch 199, loss 0.92489, train_acc 0.9776, valid_acc 0.9347, Time 00:03:13,lr 0.001\n"
     ]
    }
   ],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 200, 0.1, 5e-4\n",
    "lr_period, lr_decay, net = 80, 0.1, get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the Testing Set and Submit Results on Kaggle\n",
    "\n",
    "After obtaining a satisfactory model design and hyper-parameters, we use all training data sets (including validation sets) to retrain the model and classify the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After executing the above code, we will get a \"submission.csv\" file. The format of this file is consistent with the Kaggle competition requirements. \n",
    "\n",
    "## Hints to Improve Your Results\n",
    "\n",
    "* You should use the compete CIFAR-10 dataset to get meaningful results. \n",
    "* You'd better use a GPU machine to run it, otherwise it'll be quite slow. (Please DON'T FORGET to stop or terminate your instance if you are not using it, otherwise AWS will change you)\n",
    "* Change the `batch_size` and number of epochs `num_epochs` to 128 and 100, respectively. (It will take a while to run.)\n",
    "* Change to another network, such as ResNet-34 or Inception"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
